{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will start again our work to get topics from posts.  \n",
    "The reason of this change is simple : \n",
    "* There are a lot of pre-processing actions we could do to improve our initial dataset (lemmatization, group words together, filter words,...)\n",
    "* Our LDA model we used before got us an error we couldn't fix. This time, based on a new course (https://www.youtube.com/watch?v=6zm9NC9uRkk&ab_channel=PyData), we will improve our code.\n",
    "\n",
    "Let's begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>#Reactions</th>\n",
       "      <th>#Comments</th>\n",
       "      <th>Location</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Time_spent</th>\n",
       "      <th>Media_type</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>article</td>\n",
       "      <td>robert lerman  writes that achieving a healthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>none</td>\n",
       "      <td>national disability advocate  sara hart weir m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>article</td>\n",
       "      <td>exploring in this months talent management  hr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>article</td>\n",
       "      <td>i count myself fortunate to have spent time wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>article</td>\n",
       "      <td>online job platforms are a different way of wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34007</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>4005</td>\n",
       "      <td>93</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>image</td>\n",
       "      <td>igniter of the year  well i know that im an op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34008</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>1698</td>\n",
       "      <td>74</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>video</td>\n",
       "      <td>executives who prioritize the shareholder are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34009</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>661</td>\n",
       "      <td>59</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>video</td>\n",
       "      <td>like many i too have been reflecting as we nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34010</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>766</td>\n",
       "      <td>35</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>video</td>\n",
       "      <td>if you say customer first that means your empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34011</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>789</td>\n",
       "      <td>23</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>none</td>\n",
       "      <td>the small work hard to serve themselves in a b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31996 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name  #Reactions  #Comments Location  Followers  \\\n",
       "0      Nicholas Wyman          12          1  Unknown     6484.0   \n",
       "1      Nicholas Wyman          11          0  Unknown     6484.0   \n",
       "3      Nicholas Wyman          44          0  Unknown     6484.0   \n",
       "4      Nicholas Wyman          22          2  Unknown     6484.0   \n",
       "5      Nicholas Wyman          21          1  Unknown     6484.0   \n",
       "...               ...         ...        ...      ...        ...   \n",
       "34007     Simon Sinek        4005         93  Unknown  4206024.0   \n",
       "34008     Simon Sinek        1698         74  Unknown  4206024.0   \n",
       "34009     Simon Sinek         661         59  Unknown  4206024.0   \n",
       "34010     Simon Sinek         766         35  Unknown  4206024.0   \n",
       "34011     Simon Sinek         789         23  Unknown  4206024.0   \n",
       "\n",
       "         Time_spent Media_type  \\\n",
       "0         1 day ago    article   \n",
       "1        1 week ago       none   \n",
       "3      2 months ago    article   \n",
       "4      2 months ago    article   \n",
       "5      2 months ago    article   \n",
       "...             ...        ...   \n",
       "34007   4 years ago      image   \n",
       "34008   4 years ago      video   \n",
       "34009   4 years ago      video   \n",
       "34010   4 years ago      video   \n",
       "34011   4 years ago       none   \n",
       "\n",
       "                                                 Content  \n",
       "0      robert lerman  writes that achieving a healthy...  \n",
       "1      national disability advocate  sara hart weir m...  \n",
       "3      exploring in this months talent management  hr...  \n",
       "4      i count myself fortunate to have spent time wi...  \n",
       "5      online job platforms are a different way of wo...  \n",
       "...                                                  ...  \n",
       "34007  igniter of the year  well i know that im an op...  \n",
       "34008  executives who prioritize the shareholder are ...  \n",
       "34009  like many i too have been reflecting as we nea...  \n",
       "34010  if you say customer first that means your empl...  \n",
       "34011  the small work hard to serve themselves in a b...  \n",
       "\n",
       "[31996 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"contentCorpus.pkl\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's devide this dataset into two datasets : Corpus only & Informations about the post\n",
    "\n",
    "# We create a specific ID for each row\n",
    "data[\"ID\"]=range(data.shape[0])\n",
    "\n",
    "# We create the dataset containing content\n",
    "corpus = data[['ID','Content']]\n",
    "\n",
    "# And the one containing reactions & comments about each post\n",
    "# We don't need other columns for this analysis\n",
    "postReactions = data[['ID','#Reactions','#Comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national disability advocate  sara hart weir ms   shares how congress passed the able act\n",
      "--------------\n",
      "for those intersted in youth career pathways great to read today about the expansion of citi foundationâ€™s pathways to progress inititiave  new commitment to  young adults jobready jobs training  joanne gedge   janet searle   louise martin lindsay   amylou cowdroyling    \n",
      "--------------\n",
      "community building has meant something dramatically different the past couple of months  when  startups hosted an event with  rsvps we had to pivot almost  times to accommodate changing restrictions along the way  people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months  all these things have made me ask the question whats the best way to build community right now how is this changing our perception of community thus im starting my letter to the community on the subject \n",
      "--------------\n",
      "where can we find  casestudies  of  startups  that were built by a  team  of  entrepreneurs  virtually\n",
      "--------------\n",
      "im back in the us inspired enthused and invigorated so as i sit in the atlanta airport awaiting one more flight iâ€™m reviewing my pretrip todo list explored my  country on the mamaland check interviewed a maternalhealth superhero and living legend check renewed my love affair with africa triple check and now that iâ€™m back in the land of stable internet iâ€™ll be posting more highlights and insights from my journey on  instagram  at  and here in  linkedin   btw thanks so much linkedin fam for all the kind support and compliments you shared about my posts while i was traveling you keep me inspired too natgeotraveller globalhealth \n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for c in corpus['Content'][[1,100,1000,10000,20000]]:\n",
    "    print(c)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some words can be filtered :\n",
    "* mentions (joanne gedge, janet searle,...)\n",
    "* some hashtags that are involved in sentences (natgeotraveller , globalhealth), generally at the end of the post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts Preprocessing : what we can do ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this preprocessing, we will use Spacy, which is a fast industrial-strength natural language processing (NLP) library for Python.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "\n",
    "* Tokenization\n",
    "* Text normalization, such as lowercasing, stemming/lemmatization\n",
    "* Part-of-speech tagging\n",
    "* Syntactic dependency parsing\n",
    "* Sentence boundary detection\n",
    "* Named entity recognition and annotation\n",
    "\n",
    "In the \"batteries included\" Python tradition, spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n",
    "\n",
    "* Large English vocabulary, including stopword lists\n",
    "* Token \"probabilities\"\n",
    "* Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import codecs\n",
    "\n",
    "# We need to import spacy trained pipelines, which support many languages\n",
    "# Let's use the English pipeline\n",
    "\n",
    "# In CMD :\n",
    "# $ python -m spacy download en_core_web_lg\n",
    "# or\n",
    "# In python :\n",
    "# >>> import spacy\n",
    "# >>> nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# We download the large version \"lg\" and not the small version \"sm\" here \n",
    "# This module is pretty large : ~631 MB\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sample of several posts to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national disability advocate  sara hart weir ms   shares how congress passed the able act\n",
      "\n",
      "for those intersted in youth career pathways great to read today about the expansion of citi foundationâ€™s pathways to progress inititiave  new commitment to  young adults jobready jobs training  joanne gedge   janet searle   louise martin lindsay   amylou cowdroyling    \n",
      "\n",
      "community building has meant something dramatically different the past couple of months  when  startups hosted an event with  rsvps we had to pivot almost  times to accommodate changing restrictions along the way  people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months  all these things have made me ask the question whats the best way to build community right now how is this changing our perception of community thus im starting my letter to the community on the subject \n",
      "\n",
      "where can we find  casestudies  of  startups  that were built by a  team  of  entrepreneurs  virtually\n",
      "\n",
      "im back in the us inspired enthused and invigorated so as i sit in the atlanta airport awaiting one more flight iâ€™m reviewing my pretrip todo list explored my  country on the mamaland check interviewed a maternalhealth superhero and living legend check renewed my love affair with africa triple check and now that iâ€™m back in the land of stable internet iâ€™ll be posting more highlights and insights from my journey on  instagram  at  and here in  linkedin   btw thanks so much linkedin fam for all the kind support and compliments you shared about my posts while i was traveling you keep me inspired too natgeotraveller globalhealth \n"
     ]
    }
   ],
   "source": [
    "#Posts are separated by line breaks, and gather into one string\n",
    "posts_sample = \"\\n\\n\".join(corpus.Content[[1,100,1000,10000,20000]])\n",
    "\n",
    "print(posts_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national disability advocate  sara hart weir ms   shares how congress passed the able act\n",
      "\n",
      "for those intersted in youth career pathways great to read today about the expansion of citi foundationâ€™s pathways to progress inititiave  new commitment to  young adults jobready jobs training  joanne gedge   janet searle   louise martin lindsay   amylou cowdroyling    \n",
      "\n",
      "community building has meant something dramatically different the past couple of months  when  startups hosted an event with  rsvps we had to pivot almost  times to accommodate changing restrictions along the way  people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months  all these things have made me ask the question whats the best way to build community right now how is this changing our perception of community thus im starting my letter to the community on the subject \n",
      "\n",
      "where can we find  casestudies  of  startups  that were built by a  team  of  entrepreneurs  virtually\n",
      "\n",
      "im back in the us inspired enthused and invigorated so as i sit in the atlanta airport awaiting one more flight iâ€™m reviewing my pretrip todo list explored my  country on the mamaland check interviewed a maternalhealth superhero and living legend check renewed my love affair with africa triple check and now that iâ€™m back in the land of stable internet iâ€™ll be posting more highlights and insights from my journey on  instagram  at  and here in  linkedin   btw thanks so much linkedin fam for all the kind support and compliments you shared about my posts while i was traveling you keep me inspired too natgeotraveller globalhealth \n"
     ]
    }
   ],
   "source": [
    "parsed_posts_sample = nlp(posts_sample)\n",
    "\n",
    "print(parsed_posts_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks the same ! So what happened ?  \n",
    "Let's apply some functions !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we have already removed punctuation, but it is working very well !!!  \n",
    "I keep that here for a later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "national disability advocate  sara hart weir ms   shares how congress passed the able act\n",
      "\n",
      "for those intersted in youth career pathways great to read today about the expansion of citi foundationâ€™s pathways to progress inititiave  new commitment to  young adults jobready jobs training  joanne gedge   janet searle   louise martin lindsay   amylou cowdroyling    \n",
      "\n",
      "community building has meant something dramatically different the past couple of months  when  startups hosted an event with  rsvps we had to pivot almost  times to accommodate changing restrictions along the way\n",
      "\n",
      "Sentence 2\n",
      " people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months  all these things have made me ask the question whats the best way to build community right now how is this changing our perception of community thus im starting my letter to the community on the subject \n",
      "\n",
      "where can we find  casestudies  of  startups  that were built by a  team  of  entrepreneurs  virtually\n",
      "\n",
      "im back in the us inspired enthused and invigorated so as i sit in the atlanta airport awaiting one more flight iâ€™m reviewing my pretrip todo list explored my  country on the mamaland check interviewed a maternalhealth superhero and living legend check renewed my love affair with africa triple check and now that iâ€™m back in the land of stable internet iâ€™ll be posting more highlights and insights from my journey on  instagram  at  and here in  linkedin   btw thanks so much linkedin fam for all the kind support and compliments you shared about my posts while i was traveling you keep me inspired too natgeotraveller globalhealth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_posts_sample.sents):\n",
    "    print(\"Sentence {}\".format(num+1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "For those intersted in youth career pathways.\n",
      "\n",
      "Sentence 2\n",
      "Great to read today about the expansion of Citi Foundationâ€™s Pathways to Progress inititiave - new commitment to 500k young adults #JobReady #Jobs #Training  Joanne Gedge   Janet Searle   Louise Martin Lindsay   Amy-Lou Cowdroy-Ling   https://lnkd.in/g8FTr5w.\n",
      "\n",
      "Sentence 3\n",
      "\n",
      " \n",
      " \n",
      " â€¦see more\n",
      "\n",
      "Community building has meant something dramatically different the past couple of months.\n",
      "\n",
      "Sentence 4\n",
      " When 500 Startups hosted an event with 2400+ RSVPs, we had to pivot almost 8 times to accommodate changing restrictions along the way.\n",
      "\n",
      "Sentence 5\n",
      " People are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months.\n",
      "\n",
      "Sentence 6\n",
      " All these things have made me ask the question, what's the best way to build community right now?\n",
      "\n",
      "Sentence 7\n",
      "How is this changing our perception of community?\n",
      "\n",
      "Sentence 8\n",
      "Thus, I'm starting my letter to the community on the subject.\n",
      "\n",
      "Sentence 9\n",
      "\n",
      " \n",
      " \n",
      " â€¦see more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same with the original content, containing ponctuations & uppercases\n",
    "test = pd.read_pickle(\"cleaned_data.pkl\")\n",
    "test_content = test.content\n",
    "sample_test_content = \"\\n\\n\".join(test_content[[100,1000]])\n",
    "parsed_sample_test = nlp(sample_test_content)\n",
    "\n",
    "for num, sentence in enumerate(parsed_sample_test.sents):\n",
    "    print(\"Sentence {}\".format(num+1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name entity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1 congress - ORG\n",
      "Entity 2 today - DATE\n",
      "Entity 3 citi foundationâ€™s - ORG\n",
      "Entity 4 joanne gedge   janet searle   louise martin lindsay    - PERSON\n",
      "Entity 5 the past couple of months - DATE\n",
      "Entity 6 months - DATE\n",
      "Entity 7 atlanta - GPE\n",
      "Entity 8 one - CARDINAL\n",
      "Entity 9 africa - LOC\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_posts_sample.ents):\n",
    "    print(\"Entity {}\".format(num+1), entity, '-', entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ORG : organism \n",
    "* GPE : geopolitcal entity\n",
    "* LOC : location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can know which entity represents each words or group of words ! \n",
    "\n",
    "For instance, spacy knows when a group of words is a name, a location, a date ...  \n",
    "That's clearly amazing !\n",
    "\n",
    "Also, it appears that some words are badly comprehend by the algorithm like \"us\", as USA, which is here \"us\" designating we.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define if a word is an adjective, a noun, or other..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_text</th>\n",
       "      <th>Token_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disability</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advocate</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sara</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>me</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>inspired</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>too</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>globalhealth</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token_text Token_pos\n",
       "0           national       ADJ\n",
       "1         disability      NOUN\n",
       "2           advocate      NOUN\n",
       "3                        SPACE\n",
       "4               sara     PROPN\n",
       "..               ...       ...\n",
       "299               me      PRON\n",
       "300         inspired      VERB\n",
       "301              too       ADV\n",
       "302  natgeotraveller      NOUN\n",
       "303     globalhealth      NOUN\n",
       "\n",
       "[304 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_posts_sample]\n",
    "token_pos = [token.pos_ for token in parsed_posts_sample]\n",
    "\n",
    "pd.DataFrame({\"Token_text\" : token_text , \"Token_pos\" : token_pos})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization (stemming/lemmatization and shape analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization consists in transforming a word into its root. \n",
    "\n",
    "For instance, \"is\" becomes \"be\" ; \"me\" becomes \"I\" ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_text</th>\n",
       "      <th>Token_lemma</th>\n",
       "      <th>Token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>national</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disability</td>\n",
       "      <td>disability</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advocate</td>\n",
       "      <td>advocate</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sara</td>\n",
       "      <td>sara</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>me</td>\n",
       "      <td>I</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>inspired</td>\n",
       "      <td>inspire</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>too</td>\n",
       "      <td>too</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>globalhealth</td>\n",
       "      <td>globalhealth</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token_text      Token_lemma Token_shape\n",
       "0           national         national        xxxx\n",
       "1         disability       disability        xxxx\n",
       "2           advocate         advocate        xxxx\n",
       "3                                                \n",
       "4               sara             sara        xxxx\n",
       "..               ...              ...         ...\n",
       "299               me                I          xx\n",
       "300         inspired          inspire        xxxx\n",
       "301              too              too         xxx\n",
       "302  natgeotraveller  natgeotraveller        xxxx\n",
       "303     globalhealth     globalhealth        xxxx\n",
       "\n",
       "[304 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_posts_sample]\n",
    "token_shape = [token.shape_ for token in parsed_posts_sample]\n",
    "\n",
    "pd.DataFrame({'Token_text' : token_text, 'Token_lemma' : token_lemma , 'Token_shape' : token_shape})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a variety of other token-level attributes, such as the relative frequency of tokens, and whether or not a token matches any of these categories?\n",
    "\n",
    "* stopword\n",
    "* punctuation\n",
    "* whitespace\n",
    "* represents a number\n",
    "* whether or not the token is included in spaCy's default vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disability</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advocate</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sara</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>me</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>inspired</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>too</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>globalhealth</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0           national            -20.0                                          \n",
       "1         disability            -20.0                                          \n",
       "2           advocate            -20.0                                          \n",
       "3                               -20.0                            Yes           \n",
       "4               sara            -20.0                                          \n",
       "..               ...              ...   ...          ...         ...     ...   \n",
       "299               me            -20.0   Yes                                    \n",
       "300         inspired            -20.0                                          \n",
       "301              too            -20.0   Yes                                    \n",
       "302  natgeotraveller            -20.0                                          \n",
       "303     globalhealth            -20.0                                          \n",
       "\n",
       "    out of vocab.?  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3              Yes  \n",
       "4                   \n",
       "..             ...  \n",
       "299                 \n",
       "300                 \n",
       "301                 \n",
       "302            Yes  \n",
       "303            Yes  \n",
       "\n",
       "[304 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_posts_sample]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Log_probability represents the frequency of a word apparation in the text : \n",
    "    *  ~ 0 if appears often\n",
    "    * =! 0 if appears rarely \n",
    "    \n",
    "\n",
    "* Stop ? : Is this word a stop word ?\n",
    "\n",
    "* Out of vocab ? : Is this word out of the english dictionary proposed by Spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing : application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens  and  constitute a phrase is:\n",
    "\n",
    "$$ \\frac{count (AB) - count_{min}}{count(A)*count(B)} * N > treshold $$\n",
    "\n",
    " \n",
    "...where:\n",
    "\n",
    " * $count(A)$ is the number of times token $A$ appears in the corpus\n",
    " * $count(B)$ is the number of times token $B$ appears in the corpus\n",
    " * $count(AB)$ is the number of times the tokens $AB$ appear in the corpus in order\n",
    " * $N$ is the total size of the corpus vocabulary\n",
    " * $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    " * $treshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible **gensim** library to help us with phrase modeling â€” the Phrases class in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're performing phrase modeling, we'll be doing some iterative data transformation at the same time. Our roadmap for data preparation includes:\n",
    "\n",
    "* Segment text of complete reviews into sentences & normalize text\n",
    "* First-order phrase modeling  apply first-order phrase model to transform sentences\n",
    "* Second-order phrase modeling  apply second-order phrase model to transform sentences\n",
    "* Apply text normalization and second-order phrase model to text of complete reviews\n",
    "* We'll use this transformed data as the input for some higher-level modeling approaches in the following sections.\n",
    "\n",
    "First, let's define a few helper functions that we'll use for text normalization. In particular, the **lemmatized_sentence_corpus** generator function will use spaCy to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "\n",
    "    return token.is_punct or token.is_space\n",
    "            \n",
    "def corpus_cleaning(serie):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse posts,\n",
    "    lemmatize the text, remove punctuations, unconvenient whitespaces, stopwords, and names\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_post in nlp.pipe(serie):\n",
    "      yield ' '.join([token.lemma_ for token in parsed_post\n",
    "                             if not punct_space(token)\n",
    "                             if not token in nlp.Defaults.stop_words #stopwords\n",
    "                             if token.pos_ != \"PROPN\"]) #name & surname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used *yield* instead of *return*.  \n",
    "The reason is well explained here : https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "\n",
    "Long story short, *yield* returns a generator, which is an iterable, as a list or a string for instance, but does not store all the values in the memory.\n",
    "\n",
    "Generators recquire less memory than a list that is very useful if the dataset is very large !\n",
    "\n",
    "Therefore, **we can only iterate over once**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how to use spacy with a pandas, here a clearly explained article : https://towardsdatascience.com/structured-natural-language-processing-with-pandas-and-spacy-7089e66d2b10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to apply the spaCy language model to the entire collection of posts. The easiest and most computationally efficient way to do this is to use the *nlp.pipe* function. This will iterate over each document and will apply the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's apply the function *CorpusCleaning* to create a parsed list\n",
    "parsed_posts = corpus_cleaning(corpus.Content)\n",
    "type(parsed_posts)\n",
    "\n",
    "#We can print cleaned posts with \n",
    "#list(parsed_posts)\n",
    "#But it's time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples.\n",
    "\n",
    "To iterate on a generator, we can use the *itertools* package, which is a common practice with generator objects.\n",
    "\n",
    "it.islice is an iterator designed to iterate over an object. Because we can't iterate directly over a generator (can't be subscriptable), this function is pretty useful !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write that achieve a healthy future of work require employee to build skill that help they attain productive and rewarding career he note one of the most costeffective way to do this be through apprenticeship which help worker master occupation and gain professional identity and pride coudlnt agree more workbasedlearne apprenticeship read the article on urbanwire institute\n",
      "----\n",
      "national disability advocate ms share how pass the able act\n",
      "----\n",
      "explore in this month talent management hr what a company should consider to get the most out of a modern apprenticeship program thank to employer entrepreneuer for share insight on your it program why not start a program in wish you all a safe and happy festive season careerplanne apprenticeship workbasedlearne career institute dimeny\n",
      "----\n",
      "I count myself fortunate to have spend time with be the assistant secretary for policy evaluation and research in the department of labor during the his insight and innovative thinking around economic employment and training policy help many people particularly young people to reach their full potential pass away aged in midoctober the last project we be work up with my colleage be around verified resume thank to the institute report by lerman and lopr wonderful insight into the topic love to hear your thought on verifiedresume join the conversation stay safe research training\n",
      "----\n",
      "online job platform be a different way of work and build for our time even when our workplace school and international border might be close this be one example of the future of work happen right now join the conversation career coach businessandmanagement leadership futureofwork\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for post in it.islice(parsed_posts,5):\n",
    "    print(post)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**\n",
    "\n",
    "When we iterate over a generator, the value saved in it is then deleted ! \n",
    "\n",
    "That confirms the definition of a generator : we can only iterate over it once !\n",
    "\n",
    "**That also means if we want to use several times a generator, we have to recreate one for the purpose !**\n",
    "\n",
    "For instance, here, we can't use anymore the 6 first values contained in the generator !\n",
    "\n",
    "A common practice consists in iterating on a generator in this way :\n",
    "\n",
    "```\n",
    "for i in create_a_generator_function(y):   \n",
    "    print(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's group word together with the **gensim** library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae290cde4634a6443381d55bbe6f1180de26a7de00f3f0eb338834ba70f07938"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
