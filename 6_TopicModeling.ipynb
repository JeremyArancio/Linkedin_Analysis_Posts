{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we worked on Simon Sinek's posts to understand how to use the gensim library & LDA, let's generalize the method to the entire authors corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim import matutils, models\n",
    "import numpy\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the corpus\n",
    "data = pd.read_pickle(\"contentCorpus.pkl\")\n",
    "#Remove useless columns for this analysis. \n",
    "data.drop(['#Reactions','#Comments','Location','Followers','Time_spent','Media_type'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we want to define topics across all authors, we don't care about each post individually\n",
    "#Thus let's group posts content by author\n",
    "authorCorpus = pd.read_pickle('contentAuthorCorpus.pkl')\n",
    "authorCorpus.drop(['Followers'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform list of authors into an index\n",
    "authorCorpus.set_index('Name',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's transform the dataframe into a serie\n",
    "authorCorpus = authorCorpus.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's keep nouns & adjectives only\n",
    "#Let's create a function to pull out nouns & adjectives from a string of text\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name\n",
       "Nicholas Wyman               robert lerman healthy future work employees sk...\n",
       "Jonathan Wolfer              proud new feature douglass year amazing specia...\n",
       "Karen Gross                  piece i suggestions educators central part liv...\n",
       "Kaia Niambi Shivers Ph.D.    i native read low re high note wonderful ark r...\n",
       "Daniel Cohen-I'm Flyering    passion qualities t spot cv sourcers activitie...\n",
       "                                                   ...                        \n",
       "Quentin Michael Allums       career someone ’ s jog morning everyone run ’ ...\n",
       "AJ Wilcox                    i excited part webinar things advertising im s...\n",
       "Kevin O'Leary                crypto currencies winner demand digital curren...\n",
       "Amy Blaschka                 news ’ career progress isn ’ external s intern...\n",
       "Simon Sinek                  charge willing others charge people dangers or...\n",
       "Name: Content, Length: 68, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We transform our list of content with the function we have just created\n",
    "CorpusNounsAdj = authorCorpus.apply(nouns_adj)\n",
    "CorpusNounsAdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 45779)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaas</th>\n",
       "      <th>aai</th>\n",
       "      <th>aamkt</th>\n",
       "      <th>aams</th>\n",
       "      <th>aandetelevision</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaplillustrate</th>\n",
       "      <th>aarogyasetu</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>𝐲𝐨𝐮</th>\n",
       "      <th>𝗖𝗼𝗻𝗻𝗲𝗰𝘁𝗶𝗻𝗴</th>\n",
       "      <th>𝗛𝘂𝗺𝗮𝗻𝘀</th>\n",
       "      <th>𝗟𝗶𝘃𝗲</th>\n",
       "      <th>𝘼𝙍𝙀</th>\n",
       "      <th>𝘾𝙤𝙢𝙢𝙚𝙣𝙩</th>\n",
       "      <th>𝙈𝙮</th>\n",
       "      <th>𝙖𝙘𝙘𝙚𝙥𝙩𝙖𝙣𝙘𝙚</th>\n",
       "      <th>𝙗𝙚𝙡𝙤𝙬</th>\n",
       "      <th>𝙥𝙚𝙧𝙨𝙤𝙣𝙖𝙡</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nicholas Wyman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jonathan Wolfer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karen Gross</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaia Niambi Shivers Ph.D.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daniel Cohen-I'm Flyering</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quentin Michael Allums</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AJ Wilcox</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kevin O'Leary</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amy Blaschka</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Simon Sinek</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 45779 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           aaa  aaas  aai  aamkt  aams  aandetelevision  aapl  \\\n",
       "Name                                                                            \n",
       "Nicholas Wyman               0     0    0      0     0                0     0   \n",
       "Jonathan Wolfer              0     0    0      0     0                0     0   \n",
       "Karen Gross                  0     0    0      0     0                0     0   \n",
       "Kaia Niambi Shivers Ph.D.    0     0    0      0     0                0     0   \n",
       "Daniel Cohen-I'm Flyering    0     0    0      0     0                0     0   \n",
       "...                        ...   ...  ...    ...   ...              ...   ...   \n",
       "Quentin Michael Allums       0     0    0      0     0                0     0   \n",
       "AJ Wilcox                    0     0    0      0     0                0     0   \n",
       "Kevin O'Leary                0     0    0      0     0                0     0   \n",
       "Amy Blaschka                 0     0    0      0     0                0     0   \n",
       "Simon Sinek                  0     0    0      0     0                0     0   \n",
       "\n",
       "                           aaplillustrate  aarogyasetu  aaron  ...  𝐲𝐨𝐮  \\\n",
       "Name                                                           ...        \n",
       "Nicholas Wyman                          0            0      0  ...    0   \n",
       "Jonathan Wolfer                         0            0      0  ...    0   \n",
       "Karen Gross                             0            0      0  ...    0   \n",
       "Kaia Niambi Shivers Ph.D.               0            0      0  ...    0   \n",
       "Daniel Cohen-I'm Flyering               0            0      0  ...    0   \n",
       "...                                   ...          ...    ...  ...  ...   \n",
       "Quentin Michael Allums                  0            0      1  ...    0   \n",
       "AJ Wilcox                               0            0      1  ...    0   \n",
       "Kevin O'Leary                           0            0      1  ...    0   \n",
       "Amy Blaschka                            0            0      0  ...    0   \n",
       "Simon Sinek                             0            0      0  ...    0   \n",
       "\n",
       "                           𝗖𝗼𝗻𝗻𝗲𝗰𝘁𝗶𝗻𝗴  𝗛𝘂𝗺𝗮𝗻𝘀  𝗟𝗶𝘃𝗲  𝘼𝙍𝙀  𝘾𝙤𝙢𝙢𝙚𝙣𝙩  𝙈𝙮  \\\n",
       "Name                                                                    \n",
       "Nicholas Wyman                      0       0     0    0        0   0   \n",
       "Jonathan Wolfer                     0       0     0    0        0   0   \n",
       "Karen Gross                         0       0     0    0        0   0   \n",
       "Kaia Niambi Shivers Ph.D.           0       0     0    0        0   0   \n",
       "Daniel Cohen-I'm Flyering           0       0     0    0        0   0   \n",
       "...                               ...     ...   ...  ...      ...  ..   \n",
       "Quentin Michael Allums              0       0     0    0        0   0   \n",
       "AJ Wilcox                           0       0     0    1        1   1   \n",
       "Kevin O'Leary                       0       0     0    0        0   0   \n",
       "Amy Blaschka                        1       1     1    0        0   0   \n",
       "Simon Sinek                         0       0     0    0        0   0   \n",
       "\n",
       "                           𝙖𝙘𝙘𝙚𝙥𝙩𝙖𝙣𝙘𝙚  𝙗𝙚𝙡𝙤𝙬  𝙥𝙚𝙧𝙨𝙤𝙣𝙖𝙡  \n",
       "Name                                                    \n",
       "Nicholas Wyman                      0      0         0  \n",
       "Jonathan Wolfer                     0      0         0  \n",
       "Karen Gross                         0      0         0  \n",
       "Kaia Niambi Shivers Ph.D.           0      0         0  \n",
       "Daniel Cohen-I'm Flyering           0      0         0  \n",
       "...                               ...    ...       ...  \n",
       "Quentin Michael Allums              0      0         0  \n",
       "AJ Wilcox                           1      1         1  \n",
       "Kevin O'Leary                       0      0         0  \n",
       "Amy Blaschka                        0      0         0  \n",
       "Simon Sinek                         0      0         0  \n",
       "\n",
       "[68 rows x 45779 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "dataCv = cv.fit_transform(CorpusNounsAdj)\n",
    "dataDtm = pd.DataFrame(dataCv.toarray(), columns=cv.get_feature_names_out(),index=CorpusNounsAdj.index)\n",
    "\n",
    "print(dataDtm.shape)\n",
    "dataDtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(dataDtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) #It's actually a dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.204*\"aarp\" + 0.128*\"aatmanirbharapp\" + 0.127*\"abdi\" + 0.124*\"aams\" + 0.045*\"abacusagency\" + 0.044*\"abdomen\" + 0.026*\"aasciences\" + 0.023*\"aaron\" + 0.022*\"aback\" + 0.019*\"abandonment\"'),\n",
       " (1,\n",
       "  '0.068*\"abbacchi\" + 0.053*\"abadikorek\" + 0.052*\"aberrant\" + 0.049*\"aberman\" + 0.045*\"abhay\" + 0.043*\"abidjan\" + 0.041*\"aai\" + 0.040*\"abhishant\" + 0.037*\"abbass\" + 0.036*\"abhishek\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.194*\"abadikorek\" + 0.164*\"abacusagency\" + 0.114*\"abdomen\" + 0.113*\"abdus\" + 0.061*\"abhishek\" + 0.031*\"abibev\" + 0.031*\"abia\" + 0.028*\"abhisek\" + 0.017*\"abhi\" + 0.016*\"aas\"'),\n",
       " (1,\n",
       "  '0.122*\"aarp\" + 0.068*\"abbacchi\" + 0.052*\"aberrant\" + 0.048*\"aberman\" + 0.045*\"abhay\" + 0.042*\"abidjan\" + 0.039*\"abhishant\" + 0.037*\"abbass\" + 0.030*\"ab\" + 0.028*\"abadesi\"'),\n",
       " (2,\n",
       "  '0.474*\"abdi\" + 0.104*\"aasciences\" + 0.056*\"aberration\" + 0.031*\"aarogyasetu\" + 0.024*\"abe\" + 0.022*\"abagun\" + 0.019*\"aandetelevision\" + 0.017*\"aamkt\" + 0.012*\"abdication\" + 0.006*\"abhisek\"'),\n",
       " (3,\n",
       "  '0.289*\"aams\" + 0.279*\"aatmanirbharapp\" + 0.048*\"aai\" + 0.039*\"aback\" + 0.035*\"abercrombie\" + 0.033*\"abdominal\" + 0.031*\"abandonment\" + 0.028*\"abdulkader\" + 0.025*\"abadesi\" + 0.025*\"abdullah\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=5)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some words from this topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_stop_words = [\"abadesi\",\"abbi\",\"abdullah\",\"abhishant\",\"aback\",\"aarogyasetu\",\"abeid\",\"aaa\",\"abigail\",\"aams\"\n",
    "                 ,\"aberman\",\"abidjan\",\"abhishek\",\"abbass\",\"abhishant\",\"abacusagency\",\"abi\",\"ab\",\"abdi\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeremy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaas</th>\n",
       "      <th>aai</th>\n",
       "      <th>aamkt</th>\n",
       "      <th>aandetelevision</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aaplillustrate</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaronberson</th>\n",
       "      <th>aarp</th>\n",
       "      <th>aarps</th>\n",
       "      <th>...</th>\n",
       "      <th>𝐲𝐨𝐮</th>\n",
       "      <th>𝗖𝗼𝗻𝗻𝗲𝗰𝘁𝗶𝗻𝗴</th>\n",
       "      <th>𝗛𝘂𝗺𝗮𝗻𝘀</th>\n",
       "      <th>𝗟𝗶𝘃𝗲</th>\n",
       "      <th>𝘼𝙍𝙀</th>\n",
       "      <th>𝘾𝙤𝙢𝙢𝙚𝙣𝙩</th>\n",
       "      <th>𝙈𝙮</th>\n",
       "      <th>𝙖𝙘𝙘𝙚𝙥𝙩𝙖𝙣𝙘𝙚</th>\n",
       "      <th>𝙗𝙚𝙡𝙤𝙬</th>\n",
       "      <th>𝙥𝙚𝙧𝙨𝙤𝙣𝙖𝙡</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 45761 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aaas  aai  aamkt  aandetelevision  aapl  aaplillustrate  aaron  \\\n",
       "0      0    0      0                0     0               0      0   \n",
       "1      0    0      0                0     0               0      0   \n",
       "2      0    0      0                0     0               0      0   \n",
       "3      0    0      0                0     0               0      0   \n",
       "4      0    0      0                0     0               0      0   \n",
       "..   ...  ...    ...              ...   ...             ...    ...   \n",
       "63     0    0      0                0     0               0      1   \n",
       "64     0    0      0                0     0               0      1   \n",
       "65     0    0      0                0     0               0      1   \n",
       "66     0    0      0                0     0               0      0   \n",
       "67     0    0      0                0     0               0      0   \n",
       "\n",
       "    aaronberson  aarp  aarps  ...  𝐲𝐨𝐮  𝗖𝗼𝗻𝗻𝗲𝗰𝘁𝗶𝗻𝗴  𝗛𝘂𝗺𝗮𝗻𝘀  𝗟𝗶𝘃𝗲  𝘼𝙍𝙀  \\\n",
       "0             0     0      0  ...    0           0       0     0    0   \n",
       "1             0     0      0  ...    0           0       0     0    0   \n",
       "2             0     0      0  ...    0           0       0     0    0   \n",
       "3             0     0      0  ...    0           0       0     0    0   \n",
       "4             0     0      0  ...    0           0       0     0    0   \n",
       "..          ...   ...    ...  ...  ...         ...     ...   ...  ...   \n",
       "63            0     0      0  ...    0           0       0     0    0   \n",
       "64            0     0      0  ...    0           0       0     0    1   \n",
       "65            0     0      0  ...    0           0       0     0    0   \n",
       "66            0     0      0  ...    0           1       1     1    0   \n",
       "67            0     0      0  ...    0           0       0     0    0   \n",
       "\n",
       "    𝘾𝙤𝙢𝙢𝙚𝙣𝙩  𝙈𝙮  𝙖𝙘𝙘𝙚𝙥𝙩𝙖𝙣𝙘𝙚  𝙗𝙚𝙡𝙤𝙬  𝙥𝙚𝙧𝙨𝙤𝙣𝙖𝙡  \n",
       "0         0   0           0      0         0  \n",
       "1         0   0           0      0         0  \n",
       "2         0   0           0      0         0  \n",
       "3         0   0           0      0         0  \n",
       "4         0   0           0      0         0  \n",
       "..      ...  ..         ...    ...       ...  \n",
       "63        0   0           0      0         0  \n",
       "64        1   1           1      1         1  \n",
       "65        0   0           0      0         0  \n",
       "66        0   0           0      0         0  \n",
       "67        0   0           0      0         0  \n",
       "\n",
       "[68 rows x 45761 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate a document-term matrix with this correction\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(CorpusNounsAdj)\n",
    "dataDtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "dataDtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(dataDtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) #It's actually a dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.486*\"aams\" + 0.155*\"aai\" + 0.045*\"abad\" + 0.026*\"abbey\" + 0.023*\"abia\" + 0.021*\"abagun\" + 0.005*\"abcsharktank\" + 0.002*\"aarti\" + 0.001*\"aaas\" + 0.000*\"abbott\"'),\n",
       " (1,\n",
       "  '0.288*\"aarp\" + 0.195*\"abdi\" + 0.083*\"aberrant\" + 0.057*\"aaron\" + 0.046*\"aasciences\" + 0.037*\"abadesi\" + 0.030*\"aback\" + 0.030*\"abdullah\" + 0.027*\"aberration\" + 0.024*\"abbi\"'),\n",
       " (2,\n",
       "  '0.071*\"abbacchi\" + 0.063*\"abadikorek\" + 0.058*\"aberman\" + 0.054*\"abhay\" + 0.051*\"abidjan\" + 0.048*\"abhishant\" + 0.045*\"abbass\" + 0.043*\"abhishek\" + 0.037*\"ab\" + 0.036*\"abdus\"'),\n",
       " (3,\n",
       "  '0.420*\"aatmanirbharapp\" + 0.118*\"abdomen\" + 0.062*\"abercrombie\" + 0.049*\"abdominal\" + 0.046*\"abandonment\" + 0.045*\"abdulkader\" + 0.021*\"abe\" + 0.017*\"abdul\" + 0.015*\"abdullah\" + 0.008*\"abhi\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.  \n",
    "So there are a lot of words with no sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to remove words with no sense from the corpus.\n",
    "\n",
    "To do so, we will use the dictionary present in NLTK : nltk.words.\n",
    "\n",
    "It contains nearly all english words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 236736 words in this dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "\n",
    "englishWordsDic = nltk.corpus.words.words()\n",
    "print (\"There are {} words in this dictionary\".format(len(englishWordsDic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create the function to filter only english words\n",
    "def isEnglish(text):\n",
    "    '''Given a string of text, tokenize the text and check if each word is an english word.'''\n",
    "    tokenized = word_tokenize(text)\n",
    "    englishWord = [word for word in tokenized if (word in englishWordsDic) == True] \n",
    "    return ' '.join(englishWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined the function, we can apply it to the corpus.  \n",
    "However, because it take a very long time to process, we will focus on two authors content at first and apply LDA to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name\n",
       "Dale Corley    true words nathan stephens true true proud emp...\n",
       "Simon Sinek    charge willing others charge people dangers or...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusTest = CorpusNounsAdj.iloc[[6,67]]\n",
    "corpusTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name\n",
       "Dale Corley    true true true business brand true true true b...\n",
       "Simon Sinek    charge willing charge people organization orga...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusTestCleaned = corpusTest.apply(isEnglish)\n",
    "corpusTestCleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12088/340557498.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Document term matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpusTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataDtm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataDtm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "#Document term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(corpusTest)\n",
    "dataDtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "dataDtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(dataDtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) #It's actually a dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"abitofoptimism\" + 0.001*\"ability\" + 0.001*\"polarized\" + 0.001*\"possibilities\" + 0.001*\"positivity\" + 0.001*\"positive\" + 0.001*\"porqué\" + 0.001*\"popular\" + 0.001*\"poet\" + 0.001*\"players\"'),\n",
       " (1,\n",
       "  '0.001*\"abitofoptimism\" + 0.001*\"ability\" + 0.001*\"polarized\" + 0.001*\"possibilities\" + 0.001*\"positivity\" + 0.001*\"positive\" + 0.001*\"porqué\" + 0.001*\"popular\" + 0.001*\"poet\" + 0.001*\"players\"'),\n",
       " (2,\n",
       "  '0.916*\"abitofoptimism\" + 0.000*\"ability\" + 0.000*\"polarized\" + 0.000*\"possibilities\" + 0.000*\"positivity\" + 0.000*\"positive\" + 0.000*\"porqué\" + 0.000*\"popular\" + 0.000*\"poet\" + 0.000*\"players\"'),\n",
       " (3,\n",
       "  '0.077*\"ability\" + 0.001*\"abitofoptimism\" + 0.001*\"polarized\" + 0.001*\"possibilities\" + 0.001*\"positivity\" + 0.001*\"positive\" + 0.001*\"porqué\" + 0.001*\"popular\" + 0.001*\"poet\" + 0.001*\"players\"')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=20)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"abitofoptimism\" in englishWordsDic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA algorithm doesn't work.  \n",
    "It's probably due to the fact that we have only few document (we gather posts by author).  \n",
    "So let's use each post individually and do LDA on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4th attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robert lerman  writes that achieving a healthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>national disability advocate  sara hart weir m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exploring in this months talent management  hr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i count myself fortunate to have spent time wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>online job platforms are a different way of wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34007</th>\n",
       "      <td>igniter of the year  well i know that im an op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34008</th>\n",
       "      <td>executives who prioritize the shareholder are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34009</th>\n",
       "      <td>like many i too have been reflecting as we nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34010</th>\n",
       "      <td>if you say customer first that means your empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34011</th>\n",
       "      <td>the small work hard to serve themselves in a b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31996 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Content\n",
       "0      robert lerman  writes that achieving a healthy...\n",
       "1      national disability advocate  sara hart weir m...\n",
       "3      exploring in this months talent management  hr...\n",
       "4      i count myself fortunate to have spent time wi...\n",
       "5      online job platforms are a different way of wo...\n",
       "...                                                  ...\n",
       "34007  igniter of the year  well i know that im an op...\n",
       "34008  executives who prioritize the shareholder are ...\n",
       "34009  like many i too have been reflecting as we nea...\n",
       "34010  if you say customer first that means your empl...\n",
       "34011  the small work hard to serve themselves in a b...\n",
       "\n",
       "[31996 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the corpus\n",
    "contentCorpus = pd.read_pickle(\"contentCorpus.pkl\")\n",
    "#Remove useless columns for this analysis. \n",
    "contentCorpus.drop(['Name','#Reactions','#Comments','Location','Followers','Time_spent','Media_type'],axis=1,inplace=True)\n",
    "contentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8564/3887627496.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0menglishWordsDic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"There are {} words in this dictionary\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menglishWordsDic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "englishWordsDic = nltk.corpus.words.words()\n",
    "print (\"There are {} words in this dictionary\".format(len(englishWordsDic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the function to filter only english nouns & adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's keep nouns & adjectives only\n",
    "#Let's create a function to pull out nouns & adjectives from a string of text\n",
    "\n",
    "def corpusCleaning(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives belonging to english dictionary.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    is_english = lambda word : word in englishWordsDic\n",
    "    tokenized = word_tokenize(text)\n",
    "    cleanedtext = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos) and is_english(word)] \n",
    "    return ' '.join(cleanedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        robert lerman  writes that achieving a healthy...\n",
       "1        national disability advocate  sara hart weir m...\n",
       "3        exploring in this months talent management  hr...\n",
       "4        i count myself fortunate to have spent time wi...\n",
       "5        online job platforms are a different way of wo...\n",
       "                               ...                        \n",
       "34007    igniter of the year  well i know that im an op...\n",
       "34008    executives who prioritize the shareholder are ...\n",
       "34009    like many i too have been reflecting as we nea...\n",
       "34010    if you say customer first that means your empl...\n",
       "34011    the small work hard to serve themselves in a b...\n",
       "Name: Content, Length: 31996, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform the corpus into a Serie\n",
    "contentCorpus = contentCorpus.squeeze()\n",
    "contentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     exploring in this months talent management  hr...\n",
       "4     i count myself fortunate to have spent time wi...\n",
       "5     online job platforms are a different way of wo...\n",
       "6     between the burgeoning unemployment rates and ...\n",
       "7     this years national apprenticeship week comes ...\n",
       "8     coaching   learninganddevelopment   workforcet...\n",
       "9     look forward to joining the conversation with ...\n",
       "10    learning pods aren’t just a group of people co...\n",
       "11    congratulations to dr mark goulston on recogni...\n",
       "12    i wanted to hear from companies that take out ...\n",
       "13    lexington ky — the us bureau of labor statisti...\n",
       "14    agree with  zach boren  johns post highlights ...\n",
       "15    its been a bumpy road to a  recovery countries...\n",
       "16    the rapidly changing nature of work requires b...\n",
       "17    recessions are tough on everyone but they hit ...\n",
       "18    the imf predicts we will be entering the worst...\n",
       "19              some  practicaltips  from kathryn vasel\n",
       "21    the health care sector is multifaceted and com...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testCorpus = contentCorpus[2:20]\n",
    "testCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     talent management company modern apprenticeshi...\n",
       "4     i count time packer assistant secretary policy...\n",
       "5     job different way times international example ...\n",
       "6     unemployment stillness more more right time covid\n",
       "7     national apprenticeship week time creative way...\n",
       "8                                                      \n",
       "9     forward conversation upcoming summit question ...\n",
       "10    t group people everyone focus particular proje...\n",
       "11    mark recognition series call inspiration mark ...\n",
       "12                        best diversity i couple cisco\n",
       "13    bureau labor statistics unemployment more perc...\n",
       "14                   agree post importance young people\n",
       "15    bumpy road recovery free action second wave ap...\n",
       "16    nature work creative recruitment talent develo...\n",
       "17    tough everyone young people hard youth economy...\n",
       "18    worst global recession downturn par great depr...\n",
       "19                                                     \n",
       "21    health care sector complex let pivot such pipe...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedCorpus = testCorpus.apply(corpusCleaning)\n",
    "cleanedCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.201*\"assistant\" + 0.091*\"conversation\" + 0.077*\"agree\" + 0.062*\"carter\" + 0.055*\"best\" + 0.033*\"bureau\" + 0.005*\"cisco\" + 0.004*\"administration\" + 0.004*\"apprentice\" + 0.004*\"care\"'),\n",
       " (1,\n",
       "  '0.186*\"administration\" + 0.089*\"action\" + 0.084*\"company\" + 0.057*\"business\" + 0.051*\"apprentice\" + 0.046*\"bumpy\" + 0.045*\"cisco\" + 0.040*\"coaching\" + 0.030*\"anxiety\" + 0.029*\"care\"')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We define stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "#Document term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(cleanedCorpus)\n",
    "dataDtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(dataDtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) #It's actually a dtm\n",
    "#4topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=80)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the entire dataset : contentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        healthy future work productive ways apprentice...\n",
       "1        national disability advocate hart weir congres...\n",
       "3        talent management company modern apprenticeshi...\n",
       "4        i count time packer assistant secretary policy...\n",
       "5        job different way times international example ...\n",
       "                               ...                        \n",
       "34007    igniter year optimist idealist i world differe...\n",
       "34008            shareholder coach fair weather needs team\n",
       "34009    many end year golden circle origin i share jou...\n",
       "34010                         customer least second people\n",
       "34011          small work big way big work world small way\n",
       "Name: Content, Length: 31996, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedCorpus = contentCorpus.apply(corpusCleaning)\n",
    "cleanedCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#32 minutes to process (due to \"in English Dic\")\n",
    "cleanedCorpus.to_pickle('cleanedPostsCorpus.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        healthy future work productive ways apprentice...\n",
       "1        national disability advocate hart weir congres...\n",
       "3        talent management company modern apprenticeshi...\n",
       "4        i count time packer assistant secretary policy...\n",
       "5        job different way times international example ...\n",
       "                               ...                        \n",
       "10657                   virtual accelerator investor forum\n",
       "10658                                          better seed\n",
       "10659                   virtual accelerator investor forum\n",
       "10660                         accelerator entrepreneurship\n",
       "10661                              building strategy cloud\n",
       "Name: Content, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedCorpus = cleanedCorpus.iloc[:10000]\n",
    "reducedCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 8419 is out of bounds for axis 1 with size 7608",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12088/2958360746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_counts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#It's actually a dtm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# #4topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m             self.add_lifecycle_event(\n\u001b[0;32m    522\u001b[0m                 \u001b[1;34m\"created\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m                     )\n\u001b[1;32m-> 1005\u001b[1;33m                     \u001b[0mgammat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m             \u001b[0mexpElogbetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m             \u001b[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 8419 is out of bounds for axis 1 with size 7608"
     ]
    }
   ],
   "source": [
    "#We define stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "#Document term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(reducedCorpus)\n",
    "dataDtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(dataDtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) #It's actually a dtm\n",
    "# #4topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=1, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a bug in the LDA algorithm.  \n",
    "Let's try to figure out what makes this bug occurs !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5th attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take all authors posts dataset, and apply only Nouns & Adj function on it.  \n",
    "In other words, we don't use check if a word is in the english dictionary.  \n",
    "In this way, we will see if our algorithm has a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robert lerman  writes that achieving a healthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>national disability advocate  sara hart weir m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exploring in this months talent management  hr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i count myself fortunate to have spent time wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>online job platforms are a different way of wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34007</th>\n",
       "      <td>igniter of the year  well i know that im an op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34008</th>\n",
       "      <td>executives who prioritize the shareholder are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34009</th>\n",
       "      <td>like many i too have been reflecting as we nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34010</th>\n",
       "      <td>if you say customer first that means your empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34011</th>\n",
       "      <td>the small work hard to serve themselves in a b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31996 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Content\n",
       "0      robert lerman  writes that achieving a healthy...\n",
       "1      national disability advocate  sara hart weir m...\n",
       "3      exploring in this months talent management  hr...\n",
       "4      i count myself fortunate to have spent time wi...\n",
       "5      online job platforms are a different way of wo...\n",
       "...                                                  ...\n",
       "34007  igniter of the year  well i know that im an op...\n",
       "34008  executives who prioritize the shareholder are ...\n",
       "34009  like many i too have been reflecting as we nea...\n",
       "34010  if you say customer first that means your empl...\n",
       "34011  the small work hard to serve themselves in a b...\n",
       "\n",
       "[31996 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the corpus\n",
    "contentCorpus = pd.read_pickle(\"contentCorpus.pkl\")\n",
    "#Remove useless columns for this analysis. \n",
    "contentCorpus.drop(['Name','#Reactions','#Comments','Location','Followers','Time_spent','Media_type'],axis=1,inplace=True)\n",
    "contentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 236736 words in this dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('words')\n",
    "englishWordsDic = nltk.corpus.words.words()\n",
    "print (\"There are {} words in this dictionary\".format(len(englishWordsDic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's keep nouns & adjectives only\n",
    "#Let's create a function to pull out nouns & adjectives from a string of text\n",
    "\n",
    "def corpusCleaning(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives belonging to english dictionary.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    cleanedtext = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(cleanedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        robert lerman  writes that achieving a healthy...\n",
       "1        national disability advocate  sara hart weir m...\n",
       "3        exploring in this months talent management  hr...\n",
       "4        i count myself fortunate to have spent time wi...\n",
       "5        online job platforms are a different way of wo...\n",
       "                               ...                        \n",
       "34007    igniter of the year  well i know that im an op...\n",
       "34008    executives who prioritize the shareholder are ...\n",
       "34009    like many i too have been reflecting as we nea...\n",
       "34010    if you say customer first that means your empl...\n",
       "34011    the small work hard to serve themselves in a b...\n",
       "Name: Content, Length: 31996, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform the corpus into a Serie\n",
    "contentCorpus = contentCorpus.squeeze()\n",
    "contentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        robert lerman healthy future work employees sk...\n",
       "1        national disability advocate sara hart weir ms...\n",
       "3        months talent management hr company modern app...\n",
       "4        i count time brooklynborn arnold packer arnold...\n",
       "5        online job platforms different way times workp...\n",
       "                               ...                        \n",
       "34007    igniter year optimist im idealist i world diff...\n",
       "34008    executives shareholder coach desires fair weat...\n",
       "34009    many end year years golden circle origin i sha...\n",
       "34010               customer employees least second people\n",
       "34011          small work big way big work world small way\n",
       "Name: Content, Length: 31996, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedCorpus = contentCorpus.apply(corpusCleaning)\n",
    "cleanedCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"airline\" + 0.012*\"foreseen\" + 0.010*\"airing\" + 0.008*\"ain\" + 0.006*\"aira\" + 0.006*\"aiims\" + 0.005*\"formulaic\" + 0.005*\"aicapengage\" + 0.004*\"airplane\" + 0.004*\"aislebut\"'),\n",
       " (1,\n",
       "  '0.007*\"formation\" + 0.006*\"aichess\" + 0.005*\"ai\" + 0.004*\"aid\" + 0.003*\"aibiased\" + 0.003*\"hearmetoo\" + 0.003*\"ahmed\" + 0.003*\"aihealthcare\" + 0.003*\"aim\" + 0.003*\"aibased\"'),\n",
       " (2,\n",
       "  '0.007*\"aisle\" + 0.006*\"airlines\" + 0.005*\"airtravel\" + 0.005*\"airquality\" + 0.005*\"akande\" + 0.004*\"ais\" + 0.004*\"akali\" + 0.004*\"akamai\" + 0.003*\"aithe\" + 0.003*\"airtime\"'),\n",
       " (3,\n",
       "  '0.004*\"longread\" + 0.002*\"akinlade\" + 0.002*\"procrastinate\" + 0.002*\"processors\" + 0.001*\"aissata\" + 0.001*\"planetorplastic\" + 0.001*\"flies\" + 0.001*\"manyif\" + 0.001*\"marketingevent\" + 0.001*\"poorly\"'),\n",
       " (4,\n",
       "  '0.021*\"haveagreatweek\" + 0.007*\"aibmfin\" + 0.002*\"airlineindustry\" + 0.002*\"productinno\" + 0.002*\"ahsan\" + 0.002*\"productideas\" + 0.001*\"akula\" + 0.001*\"poster\" + 0.001*\"pertain\" + 0.001*\"imagined\"'),\n",
       " (5,\n",
       "  '0.015*\"forgetting\" + 0.008*\"foretaste\" + 0.004*\"ah\" + 0.003*\"aienabled\" + 0.003*\"forties\" + 0.002*\"ahole\" + 0.001*\"opp\" + 0.001*\"flags\" + 0.001*\"ragnarok\" + 0.001*\"ou\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform the corpus into a Serie\n",
    "contentCorpus = contentCorpus.squeeze()\n",
    "contentCorpus\n",
    "#We define stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "#Document term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(cleanedCorpus)\n",
    "dataDtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(dataDtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) #It's actually a dtm\n",
    "#4topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, passes=20)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK the LDA algorithm works correctly without our english words only filter.  \n",
    "That means we made a mistake in our filter function.  \n",
    "Let's correct it !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6th attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robert lerman  writes that achieving a healthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>national disability advocate  sara hart weir m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exploring in this months talent management  hr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i count myself fortunate to have spent time wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>online job platforms are a different way of wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34007</th>\n",
       "      <td>igniter of the year  well i know that im an op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34008</th>\n",
       "      <td>executives who prioritize the shareholder are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34009</th>\n",
       "      <td>like many i too have been reflecting as we nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34010</th>\n",
       "      <td>if you say customer first that means your empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34011</th>\n",
       "      <td>the small work hard to serve themselves in a b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31996 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Content\n",
       "0      robert lerman  writes that achieving a healthy...\n",
       "1      national disability advocate  sara hart weir m...\n",
       "3      exploring in this months talent management  hr...\n",
       "4      i count myself fortunate to have spent time wi...\n",
       "5      online job platforms are a different way of wo...\n",
       "...                                                  ...\n",
       "34007  igniter of the year  well i know that im an op...\n",
       "34008  executives who prioritize the shareholder are ...\n",
       "34009  like many i too have been reflecting as we nea...\n",
       "34010  if you say customer first that means your empl...\n",
       "34011  the small work hard to serve themselves in a b...\n",
       "\n",
       "[31996 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the corpus\n",
    "contentCorpus = pd.read_pickle(\"contentCorpus.pkl\")\n",
    "#Remove useless columns for this analysis. \n",
    "contentCorpus.drop(['Name','#Reactions','#Comments','Location','Followers','Time_spent','Media_type'],axis=1,inplace=True)\n",
    "contentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 236736 words in this dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Jeremy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('words')\n",
    "englishWordsDic = nltk.corpus.words.words()\n",
    "print (\"There are {} words in this dictionary\".format(len(englishWordsDic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's keep nouns & adjectives only\n",
    "#Let's create a function to pull out nouns & adjectives from a string of text\n",
    "\n",
    "def corpusCleaning(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives belonging to english dictionary.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    is_english = lambda word : word in englishWordsDic\n",
    "    tokenized = word_tokenize(text)\n",
    "    NounsAdjList = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    #Then, we check if words are english\n",
    "    englishList = [word for word in NounsAdjList if is_english(word)]\n",
    "    return ' '.join(englishList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        robert lerman  writes that achieving a healthy...\n",
       "1        national disability advocate  sara hart weir m...\n",
       "3        exploring in this months talent management  hr...\n",
       "4        i count myself fortunate to have spent time wi...\n",
       "5        online job platforms are a different way of wo...\n",
       "                               ...                        \n",
       "34007    igniter of the year  well i know that im an op...\n",
       "34008    executives who prioritize the shareholder are ...\n",
       "34009    like many i too have been reflecting as we nea...\n",
       "34010    if you say customer first that means your empl...\n",
       "34011    the small work hard to serve themselves in a b...\n",
       "Name: Content, Length: 31996, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform the corpus into a Serie\n",
    "contentCorpus = contentCorpus.squeeze()\n",
    "contentCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        healthy future work productive ways apprentice...\n",
       "1        national disability advocate hart weir congres...\n",
       "3        talent management company modern apprenticeshi...\n",
       "4        i count time packer assistant secretary policy...\n",
       "5        job different way times international example ...\n",
       "                               ...                        \n",
       "34007    igniter year optimist idealist i world differe...\n",
       "34008            shareholder coach fair weather needs team\n",
       "34009    many end year golden circle origin i share jou...\n",
       "34010                         customer least second people\n",
       "34011          small work big way big work world small way\n",
       "Name: Content, Length: 31996, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedCorpus = contentCorpus.apply(corpusCleaning)\n",
    "cleanedCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It appears that after this cleaning, some strings are empty \"\"\n",
    "cleanedCorpus.loc[cleanedCorpus == ''].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31213"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Therefore we remove these posts\n",
    "cleaned2Corpus = cleanedCorpus.loc[cleanedCorpus != \"\"]\n",
    "cleaned2Corpus.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save this df\n",
    "cleaned2Corpus.to_pickle(\"cleanedPostsCorpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeremy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 30498 is out of bounds for axis 1 with size 12565",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12416/713448043.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_counts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#It's actually a dtm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#4topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m             self.add_lifecycle_event(\n\u001b[0;32m    522\u001b[0m                 \u001b[1;34m\"created\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m                     )\n\u001b[1;32m-> 1005\u001b[1;33m                     \u001b[0mgammat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 705\u001b[1;33m             \u001b[0mexpElogbetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m             \u001b[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 30498 is out of bounds for axis 1 with size 12565"
     ]
    }
   ],
   "source": [
    "#We define stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "#Document term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(cleaned2Corpus)\n",
    "dataDtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(dataDtm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts) #It's actually a dtm\n",
    "#4topics\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, passes=20)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 556)\t2\n",
      "  (0, 620)\t1\n",
      "  (0, 4563)\t1\n",
      "  (0, 5081)\t1\n",
      "  (0, 5394)\t1\n",
      "  (0, 5778)\t1\n",
      "  (0, 6757)\t1\n",
      "  (0, 8529)\t1\n",
      "  (0, 8584)\t1\n",
      "  (0, 8590)\t1\n",
      "  (0, 11965)\t1\n",
      "  (0, 12276)\t1\n",
      "  (0, 12439)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 108)\t1\n",
      "  (1, 211)\t1\n",
      "  (1, 2289)\t1\n",
      "  (1, 3145)\t1\n",
      "  (1, 5036)\t1\n",
      "  (1, 7255)\t1\n",
      "  (1, 12311)\t1\n",
      "  (2, 556)\t1\n",
      "  (2, 2164)\t1\n",
      "  (2, 4164)\t1\n",
      "  (2, 5012)\t1\n",
      "  :\t:\n",
      "  (31210, 1902)\t1\n",
      "  (31210, 2763)\t1\n",
      "  (31210, 3682)\t3\n",
      "  (31210, 4140)\t1\n",
      "  (31210, 4766)\t1\n",
      "  (31210, 5224)\t1\n",
      "  (31210, 5380)\t1\n",
      "  (31210, 6065)\t2\n",
      "  (31210, 6648)\t1\n",
      "  (31210, 7676)\t1\n",
      "  (31210, 8019)\t3\n",
      "  (31210, 9924)\t1\n",
      "  (31210, 11128)\t1\n",
      "  (31210, 11251)\t1\n",
      "  (31210, 12035)\t1\n",
      "  (31210, 12452)\t2\n",
      "  (31210, 12510)\t1\n",
      "  (31211, 2696)\t1\n",
      "  (31211, 8019)\t1\n",
      "  (31211, 9764)\t1\n",
      "  (31212, 1058)\t2\n",
      "  (31212, 10182)\t2\n",
      "  (31212, 12275)\t2\n",
      "  (31212, 12439)\t2\n",
      "  (31212, 12452)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae290cde4634a6443381d55bbe6f1180de26a7de00f3f0eb338834ba70f07938"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
